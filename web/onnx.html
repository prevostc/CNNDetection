<html>
  <head> </head>

  <body>
    <!-- Load ONNX.js -->
    <script src="https://cdn.jsdelivr.net/npm/onnxjs/dist/onnx.min.js"></script>
    <input type="file" onchange="onFileChange(this.files[0])" />
    <input type="button" onclick="onTriggerAnalysis()" value="Analyze" />
    <canvas id="canvas" />

    
    <!-- Code that consume ONNX.js -->
    <script>
        function onFileChange(file) {
            console.log(file)
            var canvas = document.getElementById('canvas')
            var ctx = canvas.getContext('2d');
            var img = new Image;
            img.onload = function() {
                var width = this.width;
                var height = this.height;

                canvas.width = width;
                canvas.height = height;
                ctx.drawImage(img, 0, 0);
                console.log(img)
            }
            img.src = URL.createObjectURL(file);
        }
      // create a session
      const myOnnxSession = new onnx.InferenceSession();
      // load the ONNX model file
      const loadModelPromise = myOnnxSession.loadModel("cnndetection.onnx")
      
      async function onTriggerAnalysis() {
        var canvas = document.getElementById('canvas')
        var ctx = canvas.getContext('2d');

        // perform CenterCrop 224 ang get rgba data
        var centerCropSize = 224
        var imgData = ctx.getImageData(
            Math.max(Math.floor(canvas.width / 2) - (centerCropSize / 2), 0),
            Math.max(Math.floor(canvas.height / 2) - (centerCropSize / 2), 0),
            centerCropSize, 
            centerCropSize
        ).data;
        console.log(imgData)

        //https://stackoverflow.com/a/10475622/2523414
        //imgData is now an array where every 4 places are each pixel. So [0][1][2][3] are the [r][g][b][a] of the first pixel.
        
        // we normalize and build the final data ((x - xÌ„) / s)
        var norm_mean = [0.485, 0.456, 0.406]
        var norm_std = [0.229, 0.224, 0.225]


        var model_input = new Float32Array(3 * centerCropSize * centerCropSize)
        // for each color channel
        for (var ci = 0 ; ci < 3 ; ci ++) {
            for (var rowi = 0 ; rowi < centerCropSize ; rowi++) {
                for (var coli = 0 ; coli < centerCropSize ; coli++) {
                    var idpi = (coli + rowi) * 4 /* input from canvas has an alpha channel */
                    var tensor_val_i = imgData[idpi + ci] / 255.0 /* torchvision.transforms.ToTensor */
                    var norm_tensor_val_i = (tensor_val_i - norm_mean[ci]) / norm_std[ci] /* torchvision.transforms.Normalize */
                    model_input[(ci * centerCropSize * centerCropSize) + (rowi * centerCropSize) + coli] = norm_tensor_val_i
                }
            }
        }
        console.log(model_input)
        // note that we used a trick to avoid the onnx Shape operator
        // that prevent us from having a batch size greater than 1
        // more info: https://github.com/microsoft/onnxjs/issues/84#issuecomment-461682909
        const inputs = [
            new Tensor(model_input, "float32", [1, 3, centerCropSize, centerCropSize])
        ];
        console.log(inputs)
        await loadModelPromise;
        const outputMap = await myOnnxSession.run(inputs);
        const outputTensor = outputMap.values().next().value;
        const res = outputTensor.data[0]
        //const res = 11.138656 // fake
        //const res = -31.707857 // real
        const prob = 1/(1 + Math.exp(-res)) /* sigmoid */
        console.log({res})
        console.log({prob})
      }
    </script>
  </body>
</html>